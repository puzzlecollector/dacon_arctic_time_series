{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline UNet Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import * \n",
    "from tensorflow.keras import Model, Input  \n",
    "from tensorflow.keras.models import * \n",
    "from tensorflow.keras.callbacks import *\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split \n",
    "from sklearn.metrics import f1_score\n",
    "import random \n",
    "import os \n",
    "import time \n",
    "from tqdm import tqdm \n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv') \n",
    "filenames = train['file_nm'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 398/398 [05:53<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((398, 448, 304, 60), (398, 448, 304, 24))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = []  \n",
    "y_train = [] \n",
    "\n",
    "# use the past 3 timesteps to predict the next 12 timesteps \n",
    "past = 60\n",
    "forecast = 24\n",
    "for i in tqdm(range(len(filenames)-past-forecast), position = 0, leave = True): \n",
    "    X_train_sample = np.load('train/train/'+filenames[i]) \n",
    "    X_train_sample = (X_train_sample[:,:,0]/250 + X_train_sample[:,:,1] + X_train_sample[:,:,2]).reshape((448,304,1)) \n",
    "    \n",
    "    y_train_sample = np.load('train/train/'+filenames[i+past])\n",
    "    y_train_sample = (y_train_sample[:,:,0]/250 + y_train_sample[:,:,1] + y_train_sample[:,:,2]).reshape((448,304,1)) \n",
    "    \n",
    "    for j in range(i+1, i+past+forecast):  \n",
    "        sample = np.load('train/train/' + filenames[j]) \n",
    "        image = (sample[:,:,0]/250 + sample[:,:,1] + sample[:,:,2]).reshape((448,304,1))\n",
    "        if j < i+past: \n",
    "            X_train_sample = np.concatenate([X_train_sample, image], axis = -1) \n",
    "        elif j > i+past: \n",
    "            y_train_sample = np.concatenate([y_train_sample, image], axis = -1) \n",
    "    \n",
    "    X_train.append(X_train_sample) \n",
    "    y_train.append(y_train_sample) \n",
    "    \n",
    "X_train = np.asarray(X_train) \n",
    "y_train = np.asarray(y_train) \n",
    "\n",
    "X_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred): \n",
    "    y_true = tf.cast(y_true, tf.float32) \n",
    "    y_pred = tf.cast(y_pred, tf.float32) \n",
    "    numerator = 2*tf.reduce_sum(y_true * y_pred) \n",
    "    denominator = tf.reduce_sum(y_true + y_pred + 1e-2)     \n",
    "    return numerator / denominator \n",
    "\n",
    "def custom_loss(y_true, y_pred): \n",
    "    y_true = tf.cast(y_true, tf.float32) \n",
    "    y_pred = tf.cast(y_pred, tf.float32) \n",
    "    mae = tf.reduce_mean(tf.abs(y_true - y_pred)) \n",
    "    dice = dice_loss(y_true, y_pred) \n",
    "    return mae / dice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(input_layer, start_neurons = 32): \n",
    "    bn = BatchNormalization()(input_layer) # preprocessing step \n",
    "    conv1 = Conv2D(start_neurons * 1, (5, 5), activation = 'relu', padding = 'same')(bn) \n",
    "    conv1 = BatchNormalization()(conv1) \n",
    "    pool1 = MaxPooling2D((2,2))(conv1) \n",
    "    pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation = 'relu', padding = 'same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2,2))(conv2)  \n",
    "    pool2 = Dropout(0.25)(pool2)\n",
    "\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation = 'relu', padding = 'same')(pool2) \n",
    "    conv3 = BatchNormalization()(conv3) \n",
    "    pool3 = MaxPooling2D((2,2))(conv3) \n",
    "    pool3 = Dropout(0.25)(pool3)  \n",
    "\n",
    "    convm = Conv2D(start_neurons * 8, (3, 3), activation = 'relu', padding = 'same')(pool3) \n",
    "\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3,3), strides = (2,2), activation = 'relu', padding = 'same')(convm) \n",
    "    uconv3 = concatenate([deconv3, conv3]) \n",
    "    uconv3 = Conv2D(start_neurons * 4, (3,3), activation = 'relu', padding = 'same')(uconv3) \n",
    "    uconv3 = BatchNormalization()(uconv3) \n",
    "    uconv3 = Dropout(0.25)(uconv3) \n",
    "\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3,3), strides = (2,2), activation = 'relu', padding = 'same')(uconv3) \n",
    "    uconv2 = concatenate([deconv2, conv2]) \n",
    "    uconv2 = Conv2D(start_neurons * 2, (3,3), activation = 'relu', padding = 'same')(uconv2) \n",
    "    uconv2 = BatchNormalization()(uconv2) \n",
    "    uconv2 = Dropout(0.25)(uconv2) \n",
    "\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3,3), strides = (2,2), activation = 'relu',  padding = 'same')(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1]) \n",
    "    uconv1 = Conv2D(start_neurons * 1, (3,3), activation = 'relu', padding = 'same')(uconv1) \n",
    "    uconv1 = BatchNormalization()(uconv1) \n",
    "    uconv1 = Dropout(0.25)(uconv1) \n",
    "\n",
    "    outputs = Conv2D(24, (1,1), activation = 'relu', padding = 'same')(uconv1) \n",
    "\n",
    "    model = Model(inputs = input_layer, outputs = outputs)\n",
    "\n",
    "    model.compile(loss = custom_loss, optimizer = 'adam')\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 44s 1s/step - loss: 0.6793 - val_loss: 0.5677\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56767, saving model to models/epoch_001_val_0.568.h5\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 8s 661ms/step - loss: 0.3061 - val_loss: 0.2849\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56767 to 0.28492, saving model to models/epoch_002_val_0.285.h5\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 9s 781ms/step - loss: 0.2197 - val_loss: 0.1531\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.28492 to 0.15312, saving model to models/epoch_003_val_0.153.h5\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.1608 - val_loss: 0.1513\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15312 to 0.15130, saving model to models/epoch_004_val_0.151.h5\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.1339 - val_loss: 0.4046\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.15130\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 18s 1s/step - loss: 0.1268 - val_loss: 0.1554\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15130\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 17s 1s/step - loss: 0.1082 - val_loss: 0.1477\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15130 to 0.14768, saving model to models/epoch_007_val_0.148.h5\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.1066 - val_loss: 0.1482\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.14768\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0996 - val_loss: 0.1364\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14768 to 0.13640, saving model to models/epoch_009_val_0.136.h5\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0966 - val_loss: 0.1322\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.13640 to 0.13218, saving model to models/epoch_010_val_0.132.h5\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0979 - val_loss: 0.1287\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.13218 to 0.12875, saving model to models/epoch_011_val_0.129.h5\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0970 - val_loss: 0.1340\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.12875\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0988 - val_loss: 0.1425\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.12875\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0937 - val_loss: 0.1245\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.12875 to 0.12451, saving model to models/epoch_014_val_0.125.h5\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 12s 988ms/step - loss: 0.0908 - val_loss: 0.1211\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.12451 to 0.12106, saving model to models/epoch_015_val_0.121.h5\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0950 - val_loss: 0.1156\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.12106 to 0.11560, saving model to models/epoch_016_val_0.116.h5\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0983 - val_loss: 0.1026\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.11560 to 0.10257, saving model to models/epoch_017_val_0.103.h5\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0917 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.10257 to 0.09092, saving model to models/epoch_018_val_0.091.h5\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0873 - val_loss: 0.0849\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09092 to 0.08491, saving model to models/epoch_019_val_0.085.h5\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0839 - val_loss: 0.0808\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08491 to 0.08082, saving model to models/epoch_020_val_0.081.h5\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0815 - val_loss: 0.0762\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08082 to 0.07623, saving model to models/epoch_021_val_0.076.h5\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 12s 1s/step - loss: 0.0798 - val_loss: 0.0687\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.07623 to 0.06867, saving model to models/epoch_022_val_0.069.h5\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0801 - val_loss: 0.0685\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.06867 to 0.06852, saving model to models/epoch_023_val_0.069.h5\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0796 - val_loss: 0.0635\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06852 to 0.06354, saving model to models/epoch_024_val_0.064.h5\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 12s 969ms/step - loss: 0.0778 - val_loss: 0.0640\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06354\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0763 - val_loss: 0.0565\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.06354 to 0.05654, saving model to models/epoch_026_val_0.057.h5\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0753 - val_loss: 0.0576\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.05654\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0743 - val_loss: 0.0497\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.05654 to 0.04974, saving model to models/epoch_028_val_0.050.h5\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0738 - val_loss: 0.0509\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.04974\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0730 - val_loss: 0.0469\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.04974 to 0.04688, saving model to models/epoch_030_val_0.047.h5\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0726 - val_loss: 0.0446\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.04688 to 0.04461, saving model to models/epoch_031_val_0.045.h5\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0721 - val_loss: 0.0449\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.04461\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0717 - val_loss: 0.0417\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.04461 to 0.04168, saving model to models/epoch_033_val_0.042.h5\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0711 - val_loss: 0.0417\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.04168\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0708 - val_loss: 0.0407\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.04168 to 0.04072, saving model to models/epoch_035_val_0.041.h5\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 18s 1s/step - loss: 0.0706 - val_loss: 0.0393\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.04072 to 0.03928, saving model to models/epoch_036_val_0.039.h5\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0704 - val_loss: 0.0394\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.03928\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0701 - val_loss: 0.0381\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.03928 to 0.03809, saving model to models/epoch_038_val_0.038.h5\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0698 - val_loss: 0.0386\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03809\n",
      "Epoch 40/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 14s 1s/step - loss: 0.0695 - val_loss: 0.0378\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.03809 to 0.03785, saving model to models/epoch_040_val_0.038.h5\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0696 - val_loss: 0.0373\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.03785 to 0.03727, saving model to models/epoch_041_val_0.037.h5\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 13s 1s/step - loss: 0.0699 - val_loss: 0.0384\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03727\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0706 - val_loss: 0.0387\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03727\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0703 - val_loss: 0.0382\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03727\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0701 - val_loss: 0.0378\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 2.8147498960606756e-05.\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03727\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0699 - val_loss: 0.0375\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.25179988774471e-05.\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03727\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0698 - val_loss: 0.0374\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.8014399392995985e-05.\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03727\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0697 - val_loss: 0.0373\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.4411519805435093e-05.\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03727\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0697 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.1529216135386379e-05.\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.03727 to 0.03722, saving model to models/epoch_049_val_0.037.h5\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0697 - val_loss: 0.0371\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.03722 to 0.03714, saving model to models/epoch_050_val_0.037.h5\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0695 - val_loss: 0.0371\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.223372762789951e-06.\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03714 to 0.03710, saving model to models/epoch_051_val_0.037.h5\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0692 - val_loss: 0.0371\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 7.378698501270265e-06.\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03710\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0693 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 5.902958946535364e-06.\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03710\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0693 - val_loss: 0.0371\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 4.7223671572282915e-06.\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03710\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0694 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 3.7778936530230567e-06.\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03710\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0693 - val_loss: 0.0371\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 3.0223149224184457e-06.\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03710\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0693 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 2.4178520106943328e-06.\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03710\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0693 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.9342816813150422e-06.\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03710\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 15s 1s/step - loss: 0.0692 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.547425381431822e-06.\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.03710\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 14s 1s/step - loss: 0.0694 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.2379403415252455e-06.\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03710\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 16s 1s/step - loss: 0.0693 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 9.903522368404083e-07.\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03710\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7062d6911126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     callbacks = [learning_rate_reduction, checkpoint, early_stopping])\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_layer = Input((448, 304, 60)) \n",
    "model = UNet(input_layer)\n",
    "\n",
    "model_path = 'models/epoch_{epoch:03d}_val_{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "learning_rate_reduction = ReduceLROnPlateau(moniitor = 'val_loss', patience = 1, verbose = 1, factor = 0.8)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5) \n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs = 200,\n",
    "                    batch_size = 32,  \n",
    "                    validation_split = 0.1, \n",
    "                    callbacks = [learning_rate_reduction, checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('models/epoch_051_val_0.037.h5', custom_objects = {'custom_loss':custom_loss})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 448, 304, 60 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 448, 304, 60) 240         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 448, 304, 32) 48032       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 448, 304, 32) 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 224, 152, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 224, 152, 32) 0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 224, 152, 64) 18496       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 224, 152, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 112, 76, 64)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 112, 76, 64)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 112, 76, 128) 73856       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 112, 76, 128) 512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 56, 38, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 56, 38, 128)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 56, 38, 256)  295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 112, 76, 128) 295040      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 112, 76, 256) 0           conv2d_transpose[0][0]           \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 112, 76, 128) 295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 112, 76, 128) 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 112, 76, 128) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 224, 152, 64) 73792       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 224, 152, 128 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 224, 152, 64) 73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 224, 152, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 224, 152, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 448, 304, 32) 18464       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 448, 304, 64) 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 448, 304, 32) 18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 448, 304, 32) 128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 448, 304, 32) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 448, 304, 24) 792         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,212,968\n",
      "Trainable params: 1,211,952\n",
      "Non-trainable params: 1,016\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model summary \n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:00<00:00, 453.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(448, 304, 60)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.load('train/train/' + filenames[len(filenames)-60])[:,:,0].reshape((448,304,1)) \n",
    "\n",
    "for i in tqdm(range(len(filenames)-59, len(filenames)), position = 0, leave = True): \n",
    "    sample = np.load('train/train/' + filenames[i])\n",
    "    image = sample[:,:,0].reshape((448,304,1)) \n",
    "    X_test = np.concatenate([X_test, image], axis = -1) \n",
    "    \n",
    "\n",
    "X_test = np.asarray(X_test) \n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## multi step prediction \n",
    "X_test = X_test.reshape((1,448,304,60))\n",
    "preds = best_model.predict(X_test) # first prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds *= 250.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 136192)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = preds.reshape((24,448*304))\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv') \n",
    "sample_submission.iloc[:,1:] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('UNet_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
